{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/colabs_for_nlp/blob/main/3_word_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUKJJyV4EKwh"
      },
      "source": [
        "# <center>**Word Vectors**</center>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Definition**"
      ],
      "metadata": {
        "id": "lrfpDSkzzK6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try and imagine language as a cloud, with scattered points, where each point is a different word. The location of each point is dependent on the location of every other point in the cloud (eg. if two words share the same context, they should appear near one to another). As long as you can represent a point in space, it gets a computational representation : it becomes a vector in space, a direction. And it becomes possible to compute things from it."
      ],
      "metadata": {
        "id": "RPF5Vq2VzQUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo(\"ORstNrlG_2g\", width=512, height=288)"
      ],
      "metadata": {
        "id": "fMH5QvWSzXjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1FsTcOQ5LVgbDqkT5nm_gve5gZfQrZ8pV)"
      ],
      "metadata": {
        "id": "kkeuuYTFze23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the few modules you're going to need. Basically, here's what they do.\n",
        "<br>`os` and `glob` are useful for navigating in your content.\n",
        "\n",
        "*   `os` and `glob` are useful for navigating in your content.\n",
        "*   `gensim` is a module that contains loads of practical tools for basic word vectorization.\n",
        "*   `nltk` is useful here for string manipulation (split in sentences and so on).\n",
        "*   `lxml` is a module for interpreting xml files.\n",
        "*   `string` is a module for basic manipulation.\n",
        "*   `numpy` and `pandas` are generally used for table and matrix manipulations and representations.\n",
        "*   `matplotlib` is a representation/visualization tool."
      ],
      "metadata": {
        "id": "RkyvOkbXsfrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "6YDFgXvZLiiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btSJGNjQEKwj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import glob\n",
        "import nltk\n",
        "\n",
        "from lxml import etree as ET\n",
        "import lxml.html\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we're going to download a default folder for vector analysis. These texts are in Frantext format. Later on in the notebook, you'll be able to do the same thing with `.txt` files."
      ],
      "metadata": {
        "id": "nsuqGVPduHu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/enexdi2025_prep/refs/heads/main/auteurs.zip"
      ],
      "metadata": {
        "id": "s8-gi9VwSMEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/auteurs.zip\""
      ],
      "metadata": {
        "id": "k3JLxuWlS_jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the authors proposed by default."
      ],
      "metadata": {
        "id": "RDklwrz0urPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt2CYBKmEKwl"
      },
      "outputs": [],
      "source": [
        "flaubert=\"auteurs/flaubert/\"\n",
        "balzac=\"auteurs/balzac/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is basic (but be careful, it's dirty) code for clearing xml code."
      ],
      "metadata": {
        "id": "Mw5s-Ic3uv1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG-aW6vOEKwp"
      },
      "outputs": [],
      "source": [
        "def strip_ns_prefix(tree):\n",
        "    query = \"descendant-or-self::*[namespace-uri()!='']\"\n",
        "    for element in tree.xpath(query):\n",
        "        element.tag = ET.QName(element).localname\n",
        "    return tree"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to get each lemma from each word, store it in their sentence, and store each sentence in a list."
      ],
      "metadata": {
        "id": "FjYCOZ8hu-Ht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4tLGaUjEKwp"
      },
      "outputs": [],
      "source": [
        "if balzac != \"\":\n",
        "    files = glob.iglob(balzac + '/**/*.xml', recursive=True)\n",
        "    sentences = []\n",
        "\n",
        "    for filename in files:\n",
        "        print(filename)\n",
        "        parser = ET.XMLParser(remove_blank_text=True, resolve_entities=False, encoding='utf8')\n",
        "        tree = strip_ns_prefix(ET.parse(filename, parser))\n",
        "\n",
        "        words = tree.xpath(\".//wf/@lemma\")\n",
        "\n",
        "        sentence = []\n",
        "        for word in words:\n",
        "            if word != \".\":\n",
        "                sentence.append(word)\n",
        "            else:\n",
        "                sentences.append(sentence + [word])\n",
        "                sentence = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to check wether it worked (something should be printed)."
      ],
      "metadata": {
        "id": "uKkzOqYjvQJD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWKGOPhgEKwq"
      },
      "outputs": [],
      "source": [
        "print(len(sentences))\n",
        "print(sentences[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Xl4zaIEKwr"
      },
      "source": [
        "## Building a model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part, depending on the amount of data you intend to compute, may take some time (default : 8 minutes)"
      ],
      "metadata": {
        "id": "XLU6gj_Afxmk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RatPiNJ2EKws"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(sentences, min_count=2, max_vocab_size=10000, negative=10, epochs=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4yuImONEKws"
      },
      "outputs": [],
      "source": [
        "model.wv.save(\"/content/model_balzac.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next cell is to be run only if you want to reload a saved model."
      ],
      "metadata": {
        "id": "nCOsjlH0QC48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "KeyedVectors.load(\"/content/model_balzac.bin\")\n",
        "wv = KeyedVectors.load(\"/content/model_balzac.bin\")\n",
        "\n",
        "model = Word2Vec(vector_size=wv.vector_size, min_count=1)\n",
        "model.wv = wv"
      ],
      "metadata": {
        "id": "QAktcdnKNPpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell shows analogies between vectors : tell me, if I give you the link between \"queen\" and \"king\", the equivalent for \"man\", and you should get something like \"woman\" or \"girl\", depending on your corpus."
      ],
      "metadata": {
        "id": "20SaPPldvd8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czf2ZjP9EKws"
      },
      "outputs": [],
      "source": [
        "#Paris is to France what London is to what ? model.wv.most_similar(positive=['Londres', 'France'], negative=['Paris'],topn=5)\n",
        "#King is to man what Queen is to what ? model.wv.most_similar(positive=['reine', 'homme'], negative=['roi'],topn=5)\n",
        "model.wv.most_similar(positive=['reine', 'acteur'], negative=['roi'],topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can change the default value `'esprit'` here."
      ],
      "metadata": {
        "id": "PQQiKaTIv-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('femme',topn=20)"
      ],
      "metadata": {
        "id": "M3poyzGldb4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization with Tensorflow\n",
        "You can get a clearer visualization using the [online tensorflow visualizer](https://projector.tensorflow.org/). After this next cell, you'll get two files, one containing the vectors, the other their labels."
      ],
      "metadata": {
        "id": "dS_L_ZjGvDAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/enexdi2025_prep/refs/heads/main/stopwords_fr.txt"
      ],
      "metadata": {
        "id": "ePwXdpLrU3JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stops = open(\"/content/stopwords_fr.txt\", encoding=\"utf-8\").read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "hcgp0bLSU47L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/vecteurs.tsv\", 'w') as file_vectors, open(\"/content/metadonnees.tsv\", 'w') as file_metadata:\n",
        "    for word in model.wv.index_to_key:\n",
        "        file_vectors.write('\\t'.join([str(x) for x in model.wv[word]]) + \"\\n\")\n",
        "        file_metadata.write(word + \"\\n\")"
      ],
      "metadata": {
        "id": "IRRnV7TbVlI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Same thing with your own TXT"
      ],
      "metadata": {
        "id": "G68iG-oGwIpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we don't have pre-processed text, we need to preprocess it a bit. We'll use `stanza` for lemmatization."
      ],
      "metadata": {
        "id": "zxBfdJF-xWwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "0zSKqbFWwNi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also use a list of stopwords from a basic repo (several languages) : you can get other lists from [here](https://github.com/stopwords-iso). You'll have to look for your language, and get the `.txt` file in the adequate folder. When you see it, visualize it in \"raw\", and copy the url (and paste it in the next cell)."
      ],
      "metadata": {
        "id": "06ABwLoGxjgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/refs/heads/master/stopwords-fr.txt"
      ],
      "metadata": {
        "id": "KIEGfTEgwZ5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = open(\"/content/stopwords-fr.txt\",'r',encoding=\"utf8\").read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "C37Oxfujwh_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we're going to upload your own files."
      ],
      "metadata": {
        "id": "dM48GwBDyoiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "AGL7yblspe58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_one(dest_dir=\"/content\", rename_to=None, allow_utf8_bom=True):\n",
        "    uploaded = files.upload()\n",
        "    if len(uploaded) != 1:\n",
        "        raise ValueError(f\"Please upload only one .txt file (uploaded : {len(uploaded)}).\")\n",
        "\n",
        "    filename, data = next(iter(uploaded.items()))\n",
        "    p = Path(filename)\n",
        "\n",
        "    if p.suffix.lower() != \".txt\":\n",
        "        raise ValueError(\"Please upload a file with a .txt extension\")\n",
        "\n",
        "    try:\n",
        "        if allow_utf8_bom and data.startswith(b\"\\xef\\xbb\\xbf\"):\n",
        "            text = data.decode(\"utf-8-sig\")\n",
        "        else:\n",
        "            text = data.decode(\"utf-8\")\n",
        "    except UnicodeDecodeError as e:\n",
        "        raise ValueError(\n",
        "            \"Invalid encoding : the file is not encoded in utf8. \"\n",
        "            \"Download and save it using UTF8 (eg via Sublime Text).\"\n",
        "        ) from e\n",
        "\n",
        "    dest_name = rename_to if rename_to else p.name\n",
        "    dest_path = Path(dest_dir) / dest_name\n",
        "    dest_path.write_text(text, encoding=\"utf-8\")\n",
        "\n",
        "    return str(dest_path)"
      ],
      "metadata": {
        "id": "Sh2xY5sfnfgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_of_text = upload_one()"
      ],
      "metadata": {
        "id": "Ra7Hpj7dwmWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = open(filepath_of_text, encoding=\"utf-8\").read()"
      ],
      "metadata": {
        "id": "Rbz_Jdx7wqZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the model used is French. You can choose your own model from this [list](https://stanfordnlp.github.io/stanza/performance.html). Just change `fr` in the two following cells."
      ],
      "metadata": {
        "id": "FSwMb0ulzu10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('fr')"
      ],
      "metadata": {
        "id": "FQy7AuFUwuIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_stanza = stanza.Pipeline(lang='fr', processors='tokenize,mwt,lemma, pos', use_gpu=True)"
      ],
      "metadata": {
        "id": "JZsPQQ8Kwwfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part is just a way to use the GPU better."
      ],
      "metadata": {
        "id": "IlY1UNAA0CR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from pathlib import Path\n",
        "\n",
        "PUNCT = set(string.punctuation)\n",
        "\n",
        "def iter_chunks_by_chars(paragraphs, max_chars=100_000):\n",
        "    buf, n = [], 0\n",
        "    for p in paragraphs:\n",
        "        p = p.strip()\n",
        "        if not p:\n",
        "            continue\n",
        "        if buf and n + len(p) + 1 > max_chars:\n",
        "            yield \"\\n\".join(buf)\n",
        "            buf, n = [p], len(p)\n",
        "        else:\n",
        "            buf.append(p)\n",
        "            n += len(p) + 1\n",
        "    if buf:\n",
        "        yield \"\\n\".join(buf)\n",
        "\n",
        "def lemmatize_long_text(\n",
        "    text,\n",
        "    nlp,\n",
        "    stopwords=set(),\n",
        "    max_chars=100_000,\n",
        "):\n",
        "    paragraphs = text.split(\"\\n\")\n",
        "    sentences_lemmas = []\n",
        "\n",
        "    for chunk in iter_chunks_by_chars(paragraphs, max_chars=max_chars):\n",
        "        doc = nlp(chunk)\n",
        "        for sent in doc.sentences:\n",
        "            lemmas = []\n",
        "            for w in sent.words:\n",
        "                lemma = w.lemma\n",
        "                if not lemma:\n",
        "                    continue\n",
        "                lemma = lemma.lower()\n",
        "                if lemma in stopwords:\n",
        "                    continue\n",
        "                if lemma in PUNCT:\n",
        "                    continue\n",
        "                if all(c in string.punctuation for c in lemma):\n",
        "                    continue\n",
        "                lemmas.append(lemma)\n",
        "            if lemmas:\n",
        "                sentences_lemmas.append(lemmas)\n",
        "\n",
        "    return sentences_lemmas\n"
      ],
      "metadata": {
        "id": "NyCS-Orzw5KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = lemmatize_long_text(full_text, nlp_stanza, stopwords=stopwords, max_chars=100_000)"
      ],
      "metadata": {
        "id": "RJmTG6pcw8b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences, min_count=2, max_vocab_size=10000, negative=10, epochs=300)"
      ],
      "metadata": {
        "id": "P3O6A0qSxAxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.save(\"/content/yourModel.bin\")"
      ],
      "metadata": {
        "id": "oalaIYUFxHO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('mousquetaire',topn=50)"
      ],
      "metadata": {
        "id": "RtK7iebfxNG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/vecteurs.tsv\", 'w') as file_vectors, open(\"/content/metadonnees.tsv\", 'w') as file_metadata:\n",
        "    for word in model.wv.index_to_key:\n",
        "        file_vectors.write('\\t'.join([str(x) for x in model.wv[word]]) + \"\\n\")\n",
        "        file_metadata.write(word + \"\\n\")"
      ],
      "metadata": {
        "id": "M2Rk0YSzcBx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/word2vec_export.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    z.write(\"/content/vecteurs.tsv\", arcname=\"vecteurs.tsv\")\n",
        "    z.write(\"/content/metadonnees.tsv\", arcname=\"metadonnees.tsv\")\n",
        "\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "Jnl_ROBEp-4g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}